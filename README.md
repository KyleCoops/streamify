# Streamify

A data pipeline with Kafka, Spark Streaming, dbt, Docker, Airflow, Terraform and GCP.

## Introduction 

The following has been forked from [ankurchavda](https://github.com/ankurchavda/streamify) with the original course coming from [DataTalks.club](https://www.youtube.com/live/-zpVha7bw5A?feature=share). I made some minor adaptions to increase the Quality of Life, but additional development will be coming soon so check out the next steps section.

## Description

### Objective

The project will stream events generated from a fake music streaming service (like Spotify) and create a data pipeline that consumes the real-time data. The data coming in would be similar to an event of a user listening to a song, navigating on the website, authenticating. The data would be processed in real-time and stored to the data lake periodically (every two minutes). The hourly batch job will then consume this data, apply transformations, and create the desired tables for our dashboard to generate analytics. We will try to analyze metrics like popular songs, active users, user demographics etc.

### Dataset

The dataset is generated by [Eventsim](https://github.com/Interana/eventsim) which creates replicate page requests for a fake music web site. The docker image is borrowed from [viirya's fork](https://github.com/viirya/eventsim) of it, as the original project has gone without maintenance for a few years now.

Eventsim uses song data from [Million Songs Dataset](http://millionsongdataset.com) to generate events. A [subset](http://millionsongdataset.com/pages/getting-dataset/#subset) of 10000 songs is used.

### Data Model
The output of the finished product conforms The logical model for the reporting dataset is as follows:

![streamify-architecture](images/Streamify-Logical Model.png)

### Tools & Technologies

- Cloud - [**Google Cloud Platform**](https://cloud.google.com)
- Infrastructure as Code software - [**Terraform**](https://www.terraform.io)
- Containerization - [**Docker**](https://www.docker.com), [**Docker Compose**](https://docs.docker.com/compose/)
- Stream Processing - [**Kafka**](https://kafka.apache.org), [**Spark Streaming**](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
- Orchestration - [**Airflow**](https://airflow.apache.org)
- Transformation - [**dbt**](https://www.getdbt.com)
- Data Lake - [**Google Cloud Storage**](https://cloud.google.com/storage)
- Data Warehouse - [**BigQuery**](https://cloud.google.com/bigquery)
- Data Visualization - [**Data Studio**](https://datastudio.google.com/overview)
- Language - [**Python**](https://www.python.org)

### Technology Architecture
![streamify-architecture](images/Streamify-Architecture.jpg)

### Infrastructure Architecture
TBC

### Dashboard
TBC

## Setup

### Pre-requisites
This project is built in a manner that a basic level of knowledge for docker, terrform and python is assumed. If you are unfamilar with those aspects it is advised to take a step back and learn the technologies before attempting the above or be ready to google. 

**WARNING: You will be charged for all the infrastructre setup. If you create a new account on Google Cloud you can take advantage of the free $300 credits you recieve.**



#### Google Cloud 

1. Create an account with your Google email. 
2. Setup a [project](https://console.cloud.google.com/), eg. "Streamify".
3. Setup [service account & authentication](https://cloud.google.com/docs/authentication/getting-started) for the project.
    * Grant the following roles to a service user (Viewer, Storage Admin, Storage Object Admin, BigQuery Admin).
    * Download service-account-keys (`.json`) for auth. (Keep it secure!)
    * Rename the `.json` key file to `google_credentials.json`
4. Enable the following APIs for the project:
    * https://console.cloud.google.com/apis/library/iam.googleapis.com
    * https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com
    * https://console.cloud.google.com/apis/library/logging.googleapis.com
    * https://console.cloud.google.com/apis/library/bigquery.googleapis.com
    * https://console.cloud.google.com/apis/library/compute.googleapis.com

  Please note that the above list is not exhaustive and additional APIs might need to be enabled.

4. Setup the google cloud [SDK](https://cloud.google.com/sdk/docs/quickstart) on your local setup
5. Set environment variable to point to your downloaded GCP keys:
   ```
   export GOOGLE_APPLICATION_CREDENTIALS="<path/to/your/google_credentials.json>"
  ```
6. Refresh token/session, and verify authentication
  ```
  gcloud auth application-default login
  ```

### Terraform
- Initiate terraform and download the required dependencies-
  ```
  terraform init
  ```
- Set the follow variables in the variables.tf file
  * project
  * region
  * zone 

- View the Terraform plan

  ```
  terraform plan
  ```
- Apply the infra. **Note** - Billing will start as soon as the apply is complete.

  ```bash
  terraform apply
  ```

- (Extra) SSH into your VMs, Forward Ports - [Setup](setup/ssh.md)
- Setup Kafka Compute Instance and start sending messages from Eventsim - [Setup](setup/kafka.md)
- Setup Spark Cluster for stream processing - [Setup](setup/spark.md)
- Setup Airflow on Compute Instance to trigger the hourly data pipeline - [Setup](setup/airflow.md)


### Run the stack

### Debug

If you run into issues, see if you find something in this debug [guide](setup/debug.md).

### Next Steps
The following is currently in development:
- Migrate the cloud infrastructure to Azure
- Ensure that the services can be run through docker containers
- CI/CD

A lot can still be done :).
- Choose managed Infra
  - Cloud Composer for Airflow
  - Confluent Cloud for Kafka
- Create your own VPC network
- Build dimensions and facts incrementally instead of full refresh
- Write data quality tests
- Create dimensional models for additional business processes
- Include CI/CD
- Add more visualizations